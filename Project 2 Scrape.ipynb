{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_url= 'https://www.indeed.com/jobs?q=data+analyst&l=St.+Louis%2C+MO'\n",
    "top_cities = pd.read_csv(\"topcities.csv\")\n",
    "city_list = top_cities['City'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York, NY',\n",
       " 'Los Angeles, CA',\n",
       " 'Chicago, IL',\n",
       " 'Washington, CD',\n",
       " 'San Francisco, CA',\n",
       " 'Boston, MA',\n",
       " 'Dallas, TX',\n",
       " 'Philadelphia, PA',\n",
       " 'Houston, TX',\n",
       " 'Miami, FL',\n",
       " 'Atlanta, GA',\n",
       " 'Detroit, MI',\n",
       " 'Seattle, WA',\n",
       " 'Minneapolis, MN',\n",
       " 'Cleveland, OH',\n",
       " 'Denver, CO',\n",
       " 'Orlando, FL',\n",
       " 'Portland, OR',\n",
       " 'St. Louis, MO',\n",
       " 'Pittsburgh, PA',\n",
       " 'Charlotte, NC',\n",
       " 'Sacramento, CA',\n",
       " 'Salt Lake City, UT',\n",
       " 'Kansas City, MO',\n",
       " 'Columbus, OH',\n",
       " 'Las Vegas, NV',\n",
       " 'Indianapolis, IN',\n",
       " 'Cincinnati, OH',\n",
       " 'Raleigh, NC',\n",
       " 'Milwaukee, WI']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(indeed_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no16\n",
      "no16\n",
      "no15\n",
      "no16\n",
      "comp15\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=78.0.3904.97)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-bd9064962451>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcity_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mhtml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \"\"\"\n\u001b[1;32m--> 679\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=78.0.3904.97)\n"
     ]
    }
   ],
   "source": [
    "#There is a popup asking for a membership that has no rhyme or reason to it, but will break the scraping.\n",
    "#We have only had it pop up a few times.As long as it doesn't pop up it works great\n",
    "#I think we fixed this so it clicks through the pop up - had to make the sleep timer fairly long to let the ad load\n",
    "\n",
    "\n",
    "# cities is the list of the 30 cities\n",
    "\n",
    "\n",
    "# job_title = []\n",
    "# company = []\n",
    "# href = []\n",
    "# look_up_city = []\n",
    "\n",
    "# for city in city_list:\n",
    "#     html = browser.html\n",
    "#     soup = bs(html, \"html.parser\") \n",
    "#     try:\n",
    "#         browser.click_link_by_id('popover-close-link')\n",
    "#     except:\n",
    "# #    try:\n",
    "#         time.sleep(2)\n",
    "#         browser.execute_script(f\"document.getElementById('where').value = '{city}'\")\n",
    "# #    browser.find_by_id('where').fill('Austin, TX')\n",
    "#         browser.click_link_by_id('fj')\n",
    "#         for pages in range(1,200):\n",
    "#             html = browser.html\n",
    "#             soup = bs(html, \"html.parser\")\n",
    "#             time.sleep(2)\n",
    "#             try:\n",
    "#                 browser.click_link_by_id('popover-close-link')\n",
    "#             except:\n",
    "#                 try:\n",
    "# #                     companies = soup.find_all('span',{'class':'company'})\n",
    "#                     other_shit = soup.find_all('div',{'class':'sjcl'})\n",
    "# #                     for x in range(0,16):\n",
    "# #                          company.append(other_shit[x].text.replace('\\n',''))\n",
    "#                 except:print('comp')\n",
    "#                 try:\n",
    "#                     first = soup.find('a', {'class':'jobtitle'})\n",
    "#                     href.append(first['href'])\n",
    "#                     job_title.append(first['title'])   \n",
    "#                 except:print('no1')\n",
    "#                 try:\n",
    "#                     second = soup.find_all('a', {'class':'jobtitle'})[1]\n",
    "#                     href.append(second['href'])\n",
    "#                     job_title.append(second['title'])\n",
    "#                 except:print('no2')\n",
    "#                 try:\n",
    "#                     third = soup.find_all('a', {'class':'jobtitle'})[2]\n",
    "#                     href.append(third['href'])\n",
    "#                     job_title.append(third['title'])\n",
    "#                 except:print('no3')\n",
    "#                 try:\n",
    "#                     fourth = soup.find_all('a', {'class':'jobtitle'})[3]\n",
    "#                     href.append(fourth['href'])\n",
    "#                     job_title.append(fourth['title'])\n",
    "#                 except:print('no4')\n",
    "#                 try:\n",
    "#                     fifth = soup.find_all('a', {'class':'jobtitle'})[4]\n",
    "#                     href.append(fifth['href'])\n",
    "#                     job_title.append(fifth['title'])\n",
    "#                 except:print('no5')\n",
    "#                 try:\n",
    "#                     sixth = soup.find_all('a', {'class':'jobtitle'})[5]\n",
    "#                     href.append(sixth['href'])\n",
    "#                     job_title.append(sixth['title'])\n",
    "#                 except:print('no6')\n",
    "#                 try:\n",
    "#                     seventh = soup.find_all('a', {'class':'jobtitle'})[6]\n",
    "#                     href.append(seventh['href'])\n",
    "#                     job_title.append(seventh['title'])\n",
    "#                 except:print('no7')\n",
    "#                 try:\n",
    "#                     eighth = soup.find_all('a', {'class':'jobtitle'})[7]\n",
    "#                     href.append(eighth['href'])\n",
    "#                     job_title.append(eighth['title'])\n",
    "#                 except:print('no8')\n",
    "#                 try:\n",
    "#                     ninth = soup.find_all('a', {'class':'jobtitle'})[8]\n",
    "#                     href.append(ninth['href'])\n",
    "#                     job_title.append(ninth['title'])\n",
    "#                 except:print('no9')\n",
    "#                 try:\n",
    "#                     tenth = soup.find_all('a', {'class':'jobtitle'})[9]\n",
    "#                     href.append(tenth['href'])\n",
    "#                     job_title.append(tenth['title'])\n",
    "#                 except:print('no10')\n",
    "#                 try:\n",
    "#                     eleventh = soup.find_all('a', {'class':'jobtitle'})[10]\n",
    "#                     href.append(eleventh['href'])\n",
    "#                     job_title.append(eleventh['title'])\n",
    "#                 except:print('no11')\n",
    "#                 try:\n",
    "#                     twelth = soup.find_all('a', {'class':'jobtitle'})[11]\n",
    "#                     href.append(twelth['href'])\n",
    "#                     job_title.append(twelth['title'])\n",
    "#                 except:print('no12')\n",
    "#                 try:\n",
    "#                     thirteenth = soup.find_all('a', {'class':'jobtitle'})[12]\n",
    "#                     href.append(thirteenth['href'])\n",
    "#                     job_title.append(thirteenth['title'])\n",
    "#                 except:print('no13')\n",
    "#                 try:\n",
    "#                     fourteenth = soup.find_all('a', {'class':'jobtitle'})[13]\n",
    "#                     href.append(fourteenth['href'])\n",
    "#                     job_title.append(fourteenth['title'])\n",
    "#                 except:print('no14')\n",
    "#                 try:\n",
    "#                     fifteenth = soup.find_all('a', {'class':'jobtitle'})[14]\n",
    "#                     href.append(fifteenth['href'])\n",
    "#                     job_title.append(fifteenth['title'])\n",
    "#                 except:print('no15')\n",
    "#                 try:\n",
    "#                     sixteenth = soup.find_all('a', {'class':'jobtitle'})[15]\n",
    "#                     href.append(sixteenth['href'])\n",
    "#                     job_title.append(sixteenth['title'])\n",
    "#                 except:print('no16')\n",
    "#                 try:\n",
    "#                     one = soup.find_all('div',{'class':'sjcl'})[0]\n",
    "#                     company.append(one.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(one.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(one.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except:print('comp1')\n",
    "#                 try:\n",
    "#                     two = soup.find_all('div',{'class':'sjcl'})[1]\n",
    "#                     company.append(two.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(two.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try: \n",
    "#                         look_up_city.append(two.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp2')\n",
    "#                 try:\n",
    "#                     three = soup.find_all('div',{'class':'sjcl'})[2]\n",
    "#                     company.append(three.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(three.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(three.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except:print('comp3')\n",
    "#                 try:\n",
    "#                     four = soup.find_all('div',{'class':'sjcl'})[3]\n",
    "#                     company.append(four.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(four.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(four.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except:print('comp4')\n",
    "#                 try:\n",
    "#                     five = soup.find_all('div',{'class':'sjcl'})[4]\n",
    "#                     company.append(five.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(five.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(five.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp5')\n",
    "#                 try:\n",
    "#                     six = soup.find_all('div',{'class':'sjcl'})[5]\n",
    "#                     company.append(six.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(six.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(six.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp6')\n",
    "#                 try:\n",
    "#                     seven = soup.find_all('div',{'class':'sjcl'})[6]\n",
    "#                     company.append(seven.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(seven.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(seven.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp7')\n",
    "#                 try:\n",
    "#                     eight = soup.find_all('div',{'class':'sjcl'})[7]\n",
    "#                     company.append(eight.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(eight.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(eight.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp8')\n",
    "#                 try:\n",
    "#                     nine = soup.find_all('div',{'class':'sjcl'})[8]\n",
    "#                     company.append(nine.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(nine.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(nine.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except:print('comp9')\n",
    "#                 try:\n",
    "#                     ten = soup.find_all('div',{'class':'sjcl'})[9]\n",
    "#                     company.append(ten.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(ten.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(ten.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp10')\n",
    "#                 try:\n",
    "#                     eleven = soup.find_all('div',{'class':'sjcl'})[10]\n",
    "#                     company.append(eleven.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(eleven.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(eleven.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp11')\n",
    "#                 try:\n",
    "#                     twelve = soup.find_all('div',{'class':'sjcl'})[11]\n",
    "#                     company.append(twelve.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(twelve.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try: \n",
    "#                         look_up_city.append(twelve.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp12')\n",
    "#                 try:\n",
    "#                     thirteen = soup.find_all('div',{'class':'sjcl'})[12]\n",
    "#                     company.append(thirteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(thirteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try: \n",
    "#                         look_up_city.append(thirteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except: print('comp13')\n",
    "#                 try:\n",
    "#                     fourteen = soup.find_all('div',{'class':'sjcl'})[13]\n",
    "#                     company.append(fourteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(fourteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(fourteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except:print('comp14')\n",
    "#                 try:\n",
    "#                     fifteen = soup.find_all('div',{'class':'sjcl'})[14]\n",
    "#                     company.append(fifteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(fifteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(fifteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except:print('comp15')\n",
    "#                 try:\n",
    "#                     sixteen = soup.find_all('div',{'class':'sjcl'})[15]\n",
    "#                     company.append(sixteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "#                     look_up_city.append(sixteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "#                 except:\n",
    "#                     try:\n",
    "#                         look_up_city.append(sixteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "#                     except:print('comp16')\n",
    "#                 try: \n",
    "#                     browser.find_link_by_partial_text('Next').last.click()\n",
    "#                 except: break\n",
    "                    \n",
    "\n",
    "#    except:\n",
    "#        data.append(pd.read_html(str(soup))[12])\n",
    "#        try:\n",
    "#    browser.click_link_by_text(x+1)\n",
    "#            browser.find_by_name('WARBoard1$dg1$ctl00$ctl02$ctl00$ctl18').first.click()\n",
    "#         browser.click_link_by_text(str(x))\n",
    "#        except:\n",
    "#            browser.find_by_name('WARBoard1$dg1$ctl00$ctl02$ctl00$ctl20').first.click()\n",
    "#         browser.click_link_by_text(str(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_df = pd.DataFrame(job_title, columns = ['Job'])\n",
    "# job_df.to_csv(r'job.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_df = pd.DataFrame(look_up_city, columns = ['City'])\n",
    "# city_df.to_csv(r'citylookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# href_df = pd.DataFrame(href, columns=['href'])\n",
    "# href_df.to_csv(r'href.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_df = pd.DataFrame(company, columns=['Company'])\n",
    "# company_df.to_csv(r'company.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "browser.visit(indeed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp16\n",
      "comp17\n",
      "comp18\n",
      "comp17\n",
      "comp18\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=78.0.3904.97)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-ae6d10efbd2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick_link_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'popover-close-link'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\splinter\\driver\\__init__.py\u001b[0m in \u001b[0;36mclick_link_by_id\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mfind_by_id\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mfind_by\u001b[1;34m(self, finder, selector, original_find, original_query)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                 \u001b[0melements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melements\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_id\u001b[1;34m(self, id_)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=78.0.3904.97)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-ae6d10efbd2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#    try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"document.getElementById('where').value = '{city}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m#    browser.find_by_id('where').fill('Austin, TX')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick_link_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fj'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    634\u001b[0m         return self.execute(command, {\n\u001b[0;32m    635\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             'args': converted_args})['value']\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pandas\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=78.0.3904.97)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['job_title','company','city','summary','href'])\n",
    "for city in city_list:\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\") \n",
    "    try:\n",
    "        browser.click_link_by_id('popover-close-link')\n",
    "    except:\n",
    "#    try:\n",
    "        time.sleep(2)\n",
    "        browser.execute_script(f\"document.getElementById('where').value = '{city}'\")\n",
    "#    browser.find_by_id('where').fill('Austin, TX')\n",
    "        browser.click_link_by_id('fj')\n",
    "        for pages in range(1,200):\n",
    "            html = browser.html\n",
    "            soup = bs(html, \"html.parser\")\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                browser.click_link_by_id('popover-close-link')\n",
    "            except:\n",
    "                try:\n",
    "                    row = []\n",
    "                    first = soup.find('a', {'class':'jobtitle'})\n",
    "                    one = soup.find_all('div',{'class':'sjcl'})[0]\n",
    "                    a = soup.find_all('div',{'class':'summary'})[0]\n",
    "                    row.append(first['title']) \n",
    "                    row.append(one.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(one.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(a.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{first['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(one.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(a.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{first['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp1')\n",
    "                try:\n",
    "                    row = []\n",
    "                    second = soup.find_all('a', {'class':'jobtitle'})[1]\n",
    "                    two = soup.find_all('div',{'class':'sjcl'})[1]\n",
    "                    b = soup.find_all('div',{'class':'summary'})[1]\n",
    "                    row.append(second['title']) \n",
    "                    row.append(two.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(two.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(b.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{second['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(two.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(b.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{second['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp2')                \n",
    "                try:\n",
    "                    row = []\n",
    "                    third = soup.find_all('a', {'class':'jobtitle'})[2]\n",
    "                    three = soup.find_all('div',{'class':'sjcl'})[2]\n",
    "                    c = soup.find_all('div',{'class':'summary'})[2]\n",
    "                    row.append(third['title']) \n",
    "                    row.append(three.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(three.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(c.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{third['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(three.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(c.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{third['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp3')\n",
    "                try:\n",
    "                    row = []\n",
    "                    fourth = soup.find_all('a', {'class':'jobtitle'})[3]\n",
    "                    four = soup.find_all('div',{'class':'sjcl'})[3]\n",
    "                    d = soup.find_all('div',{'class':'summary'})[3]\n",
    "                    row.append(fourth['title']) \n",
    "                    row.append(four.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(four.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(d.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{fourth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(four.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(d.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{fourth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp4')\n",
    "                try:\n",
    "                    row = []\n",
    "                    fifth = soup.find_all('a', {'class':'jobtitle'})[4]\n",
    "                    five = soup.find_all('div',{'class':'sjcl'})[4]\n",
    "                    e = soup.find_all('div',{'class':'summary'})[4]\n",
    "                    row.append(fifth['title']) \n",
    "                    row.append(five.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(five.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(e.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{fifth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(five.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(e.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{fifth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp5')\n",
    "                try:\n",
    "                    row = []\n",
    "                    sixth = soup.find_all('a', {'class':'jobtitle'})[5]\n",
    "                    six = soup.find_all('div',{'class':'sjcl'})[5]\n",
    "                    f = soup.find_all('div',{'class':'summary'})[5]\n",
    "                    row.append(sixth['title']) \n",
    "                    row.append(six.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(six.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(f.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{sixth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(six.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(f.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{sixth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except: print('comp6')\n",
    "                try:\n",
    "                    row = []\n",
    "                    seventh = soup.find_all('a', {'class':'jobtitle'})[6]\n",
    "                    seven = soup.find_all('div',{'class':'sjcl'})[6]\n",
    "                    g = soup.find_all('div',{'class':'summary'})[6]\n",
    "                    row.append(seventh['title']) \n",
    "                    row.append(seven.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(seven.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(g.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{seventh['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(seven.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(g.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{seventh['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp7')\n",
    "                try:\n",
    "                    row = []\n",
    "                    eighth = soup.find_all('a', {'class':'jobtitle'})[7]\n",
    "                    eight = soup.find_all('div',{'class':'sjcl'})[7]\n",
    "                    h = soup.find_all('div',{'class':'summary'})[7]\n",
    "                    row.append(eighth['title']) \n",
    "                    row.append(eight.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(eight.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(h.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{eighth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(eight.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(h.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{eighth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp8')\n",
    "                try:\n",
    "                    row = []\n",
    "                    ninth = soup.find_all('a', {'class':'jobtitle'})[8]\n",
    "                    nine = soup.find_all('div',{'class':'sjcl'})[8]\n",
    "                    i = soup.find_all('div',{'class':'summary'})[8]\n",
    "                    row.append(ninth['title']) \n",
    "                    row.append(nine.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(nine.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(i.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{ninth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(nine.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(i.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{ninth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp9')\n",
    "                try:\n",
    "                    row = []\n",
    "                    tenth = soup.find_all('a', {'class':'jobtitle'})[9]\n",
    "                    ten = soup.find_all('div',{'class':'sjcl'})[9]\n",
    "                    j = soup.find_all('div',{'class':'summary'})[9]\n",
    "                    row.append(tenth['title']) \n",
    "                    row.append(ten.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(ten.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(j.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{tenth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(ten.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(j.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{tenth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp10')\n",
    "                try:\n",
    "                    row = []\n",
    "                    eleventh = soup.find_all('a', {'class':'jobtitle'})[10]\n",
    "                    eleven = soup.find_all('div',{'class':'sjcl'})[10]\n",
    "                    k = soup.find_all('div',{'class':'summary'})[10]\n",
    "                    row.append(eleventh['title']) \n",
    "                    row.append(eleven.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(eleven.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(k.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{eleventh['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(eleven.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(k.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{eleventh['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp11')\n",
    "                try:\n",
    "                    row = []\n",
    "                    twelfth = soup.find_all('a', {'class':'jobtitle'})[11]\n",
    "                    twelve = soup.find_all('div',{'class':'sjcl'})[11]\n",
    "                    l = soup.find_all('div',{'class':'summary'})[11]\n",
    "                    row.append(twelfth['title']) \n",
    "                    row.append(twelve.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(twelve.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(l.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{twelfth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(twelve.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(l.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{twelfth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp12')   \n",
    "                try:\n",
    "                    row = []\n",
    "                    thirteenth = soup.find_all('a', {'class':'jobtitle'})[12]\n",
    "                    thirteen = soup.find_all('div',{'class':'sjcl'})[12]\n",
    "                    m = soup.find_all('div',{'class':'summary'})[12]\n",
    "                    row.append(thirteenth['title']) \n",
    "                    row.append(thirteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(thirteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(m.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{thirteenth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(thirteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(m.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{thirteenth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp13')   \n",
    "                try:\n",
    "                    row = []\n",
    "                    fourteenth = soup.find_all('a', {'class':'jobtitle'})[13]\n",
    "                    fourteen = soup.find_all('div',{'class':'sjcl'})[13]\n",
    "                    n = soup.find_all('div',{'class':'summary'})[13]\n",
    "                    row.append(fourteenth['title']) \n",
    "                    row.append(fourteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(fourteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(n.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{fourteenth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(fourteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(n.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{fourteenth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp14') \n",
    "                try:\n",
    "                    row = []\n",
    "                    fifteenth = soup.find_all('a', {'class':'jobtitle'})[14]\n",
    "                    fifteen = soup.find_all('div',{'class':'sjcl'})[14]\n",
    "                    o = soup.find_all('div',{'class':'summary'})[14]\n",
    "                    row.append(fifteenth['title']) \n",
    "                    row.append(fifteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(fifteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(o.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{fifteenth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(fifteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(o.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{fifteenth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp15')\n",
    "                try:\n",
    "                    row = []\n",
    "                    sixteenth = soup.find_all('a', {'class':'jobtitle'})[15]\n",
    "                    sixteen = soup.find_all('div',{'class':'sjcl'})[15]\n",
    "                    p = soup.find_all('div',{'class':'summary'})[15]\n",
    "                    row.append(sixteenth['title']) \n",
    "                    row.append(sixteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(sixteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(p.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{sixteenth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(sixteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(p.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{sixteenth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp16') \n",
    "                try:\n",
    "                    row = []\n",
    "                    seventeenth = soup.find_all('a', {'class':'jobtitle'})[16]\n",
    "                    seventeen = soup.find_all('div',{'class':'sjcl'})[16]\n",
    "                    q = soup.find_all('div',{'class':'summary'})[16]\n",
    "                    row.append(seventeenth['title']) \n",
    "                    row.append(seventeen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(seventeen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(q.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{seventeenth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(seventeen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(q.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{seventeenth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp17')\n",
    "                try:\n",
    "                    row = []\n",
    "                    eighteenth = soup.find_all('a', {'class':'jobtitle'})[17]\n",
    "                    eighteen = soup.find_all('div',{'class':'sjcl'})[17]\n",
    "                    r = soup.find_all('div',{'class':'summary'})[17]\n",
    "                    row.append(eighteenth['title']) \n",
    "                    row.append(eighteen.find('span', {'class':'company'}).text.replace('\\n',''))\n",
    "                    row.append(eighteen.find('div', {'class':'location'}).text.replace('\\n',''))\n",
    "                    row.append(r.text.replace('\\n',''))\n",
    "                    row.append(f\"indeed.com{eighteenth['href']}\")\n",
    "                    line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                    df = df.append(line)\n",
    "                except:\n",
    "                    try:\n",
    "                        row.append(eighteen.find('span', {'class':'location'}).text.replace('\\n',''))\n",
    "                        row.append(r.text.replace('\\n',''))\n",
    "                        row.append(f\"indeed.com{eighteenth['href']}\")\n",
    "                        line = pd.DataFrame([row], columns=(['job_title','company','city','summary','href']))\n",
    "                        df = df.append(line)\n",
    "                    except:print('comp18')                     \n",
    "                try: \n",
    "                    browser.find_link_by_partial_text('Next').last.click()\n",
    "                except: break\n",
    "df.to_csv(r'newdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" It comprises more than 200 locations throughout the New York area, including five inpatient locations, a children's hospital, three emergency rooms and a level\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>city</th>\n",
       "      <th>summary</th>\n",
       "      <th>href</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alcohol Products Data Mapping Analyst</td>\n",
       "      <td>3x3 Insights</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Data Mapping Analyst  Part Time.3x3 Insights...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0AOP7UQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst, Special Investigations Unit</td>\n",
       "      <td>EmblemHealth/AdvantageCare Physicians</td>\n",
       "      <td>New York, NY 10004 (Financial District area)</td>\n",
       "      <td>3-5 Years of experience in data analytics, pr...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CkHVnD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BVAL Yield Curve Analyst</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Bloomberg's evaluated pricing service, BVAL, ...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0Ab67y_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst, CHOICE</td>\n",
       "      <td>Visiting Nurse Service of New York</td>\n",
       "      <td>New York, NY 10261</td>\n",
       "      <td>Compiles, analyzes, and reports on CHOICE hea...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0B2LP6K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior Analyst, Data</td>\n",
       "      <td>HarperCollins Publishers Inc.</td>\n",
       "      <td>New York, NY 10022 (Midtown area)</td>\n",
       "      <td>HarperCollins is seeking a Junior Analyst to ...</td>\n",
       "      <td>indeed.com/rc/clk?jk=977d1aa7722eafeb&amp;fccid=6b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Peloton</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>As a Junior Data Analyst, you will join the B...</td>\n",
       "      <td>indeed.com/rc/clk?jk=737c4d597c481e55&amp;fccid=88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Full Time Job Opportunity for Data Analyst wit...</td>\n",
       "      <td>KGS Technology Group, Inc</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>*Immigration/VISA Support (International stud...</td>\n",
       "      <td>indeed.com/company/KGS-Technology-Group-Inc/jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OIRA Data Analyst</td>\n",
       "      <td>Research Foundation of The City University of ...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Candidates should have a great attention to d...</td>\n",
       "      <td>indeed.com/rc/clk?jk=5bb807b6ecf2ec25&amp;fccid=66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyst, Data Analytics</td>\n",
       "      <td>CMI/Compas</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Keen attention to detail, organization, and s...</td>\n",
       "      <td>indeed.com/rc/clk?jk=d6253d34d6fa1a48&amp;fccid=7d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Digital Data Analyst - SportsNet New York (SNY)</td>\n",
       "      <td>NBCUniversal</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>SNY is looking for a part-time data analyst t...</td>\n",
       "      <td>indeed.com/rc/clk?jk=2c7f0d512362ff79&amp;fccid=35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Turner</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>To be successful in the role, you'll need to ...</td>\n",
       "      <td>indeed.com/rc/clk?jk=dbb0ebab7b2dab52&amp;fccid=92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product Analyst, Data</td>\n",
       "      <td>Reonomy</td>\n",
       "      <td>New York, NY 10017 (Midtown area)</td>\n",
       "      <td>A detail oriented mind with inclination for i...</td>\n",
       "      <td>indeed.com/rc/clk?jk=852c306a7c769890&amp;fccid=ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst (NCU)</td>\n",
       "      <td>New York City DEPARTMENT OF CORRECTION</td>\n",
       "      <td>Queens, NY</td>\n",
       "      <td>Strong analytical skills with ability to coll...</td>\n",
       "      <td>indeed.com/rc/clk?jk=e1920b5544f31cee&amp;fccid=68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Talent Data Analyst</td>\n",
       "      <td>Uncommon Schools</td>\n",
       "      <td>New York, NY 10018 (Clinton area)</td>\n",
       "      <td>The Associate Director for Talent Data and St...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CApKCi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr. Data Analyst</td>\n",
       "      <td>Noom Inc.</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>At Noom, we use scientifically proven methods...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CTxeck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Healthcare Quality Assurance Data Analyst</td>\n",
       "      <td>Bridging Access to Care, Inc</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>Working with information systems: 3 years (Re...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0C7-oBx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sales Operations, Senior Analyst</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>You must possess strong Excel skills, a good ...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CiRNM7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst, Paid Media</td>\n",
       "      <td>Disney Streaming Services</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Disney Streaming Services (DSS) is looking fo...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0C-wIR9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audience Data &amp; Analytics Specialist</td>\n",
       "      <td>Investment News</td>\n",
       "      <td>New York, NY 10017 (Midtown area)</td>\n",
       "      <td>You are detail oriented and take pride in bei...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0DoqYCt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyst, Data &amp; Analysis</td>\n",
       "      <td>Digitas</td>\n",
       "      <td>New York, NY 10014 (West Village area)</td>\n",
       "      <td>1-3 years of work experience in a relevant in...</td>\n",
       "      <td>indeed.com/rc/clk?jk=c0708d39bef9d2eb&amp;fccid=27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>RedRoute</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>RedRoute is paving the path to amazing voice-...</td>\n",
       "      <td>indeed.com/company/RedRoute/jobs/Data-Analyst-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>UncommonGoods</td>\n",
       "      <td>Brooklyn, NY 11220 (Sunset Park area)</td>\n",
       "      <td>Analytics works with numerous departments acr...</td>\n",
       "      <td>indeed.com/rc/clk?jk=28d611e84addb020&amp;fccid=5b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst, PNA</td>\n",
       "      <td>New York City NYC HOUSING AUTHORITY</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Analyze financial and investment opportunitie...</td>\n",
       "      <td>indeed.com/rc/clk?jk=fa7fdea1e7ea4c08&amp;fccid=96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People Data Analyst</td>\n",
       "      <td>Peloton</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Excellent attention to detail, process orient...</td>\n",
       "      <td>indeed.com/rc/clk?jk=39737f1c09659831&amp;fccid=88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quantitative/Data Analyst Intern</td>\n",
       "      <td>KKI Capital</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Collect, validate and organize data per the t...</td>\n",
       "      <td>indeed.com/rc/clk?jk=018ed0ca7a7ce1a2&amp;fccid=2b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>BazaarVoice</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Superior organizational skills and attention ...</td>\n",
       "      <td>indeed.com/rc/clk?jk=5d77612f8cc80067&amp;fccid=e2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Data Analyst (Entry Level)</td>\n",
       "      <td>Medly Pharmacy</td>\n",
       "      <td>Brooklyn, NY 11206 (Williamsburg area)</td>\n",
       "      <td>Were looking for an entry-level data analyst...</td>\n",
       "      <td>indeed.com/company/Medly-Pharmacy/jobs/Busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>ThreatMetrix</td>\n",
       "      <td>New York, NY 10118 (Chelsea area)</td>\n",
       "      <td>The fraud data analyst works as part of a cus...</td>\n",
       "      <td>indeed.com/company/ThreatMETRIX/jobs/Data-Anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product Data Analyst - Equinox Media</td>\n",
       "      <td>EQUINOX</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Attention to detail and accuracy.Product Data...</td>\n",
       "      <td>indeed.com/rc/clk?jk=538ef7b5ac072ac9&amp;fccid=8e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>TeamSoft, Inc</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Policies and claims: 5 years (Preferred).Data...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CDnM5s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Integrated Reporting is Simple (IRIS CRM)</td>\n",
       "      <td>Brooklyn, NY 11209 (Bay Ridge area)</td>\n",
       "      <td>On a scale of 1 to 5, with 5 being expert, ho...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0C7ZzwE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>NYU Langone Health</td>\n",
       "      <td>New York, NY 10016 (Gramercy area)</td>\n",
       "      <td>Excellent written and oral communication skil...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0BgHAp2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Epic HIM Analyst</td>\n",
       "      <td>NYU Langone Health</td>\n",
       "      <td>New York, NY 10016 (Gramercy area)</td>\n",
       "      <td>Requires 1-3 years of relevant experience and...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0BgHAp2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Financial - Data Analyst</td>\n",
       "      <td>Renovation Brands, LLC</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>3 years experience in within a Finance or Ac...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0AspZ6W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT Business Analyst</td>\n",
       "      <td>Worldwide Flight Services</td>\n",
       "      <td>Jamaica, NY</td>\n",
       "      <td>MS Power BI: 5 years (Required).Microsoft Acc...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CFS0Jg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EpicCare Analyst</td>\n",
       "      <td>NYU Langone Health</td>\n",
       "      <td>New York, NY 10016 (Gramercy area)</td>\n",
       "      <td>Requires 1-3 years of relevant experience and...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0BgHAp2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mortgage Data &amp; Collateral Analyst</td>\n",
       "      <td>Global Atlantic Financial Group</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Ability to learn quickly with a strong attent...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0CttO8m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data &amp; Insight Analyst, DK</td>\n",
       "      <td>Penguin Random House LLC</td>\n",
       "      <td>New York, NY 10019 (Midtown area)</td>\n",
       "      <td>Excellent attention to detail and time manage...</td>\n",
       "      <td>indeed.com/rc/clk?jk=143462ad92078a27&amp;fccid=2e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investigative Analyst</td>\n",
       "      <td>New York District Attorney's Office</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Analyze documentary evidence, including cell-...</td>\n",
       "      <td>indeed.com/rc/clk?jk=e45248ae06a1eb53&amp;fccid=db...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cedar Inc</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Cedar is a patient payment and engagement pla...</td>\n",
       "      <td>indeed.com/rc/clk?jk=58a71fc749a37a28&amp;fccid=67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Gallagher</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Gallagher is a global leader in insurance, ri...</td>\n",
       "      <td>indeed.com/rc/clk?jk=91abbd950fc6d0e8&amp;fccid=34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HR DATA ANALYST</td>\n",
       "      <td>ABM Industries</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Outstanding analytical and organizational ski...</td>\n",
       "      <td>indeed.com/rc/clk?jk=5a5e37b51590202f&amp;fccid=dc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst, Customer Success</td>\n",
       "      <td>CipherHealth</td>\n",
       "      <td>New York, NY 10018 (Clinton area)</td>\n",
       "      <td>We are a fast-growing company in the healthca...</td>\n",
       "      <td>indeed.com/rc/clk?jk=87e0676abf63f477&amp;fccid=11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyst, Vantage Data Solution</td>\n",
       "      <td>Viacom</td>\n",
       "      <td>New York, NY 10036</td>\n",
       "      <td>2-5 years working experience in data analytic...</td>\n",
       "      <td>indeed.com/rc/clk?jk=fb90d6ff1a9b2346&amp;fccid=ae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst Entry Level</td>\n",
       "      <td>Endai</td>\n",
       "      <td>New York, NY 10038 (Financial District area)</td>\n",
       "      <td>Type: Paid Intern (in a farm team).Education:...</td>\n",
       "      <td>indeed.com/rc/clk?jk=a630d8604e79813a&amp;fccid=f7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product Analyst, Data Science</td>\n",
       "      <td>Google</td>\n",
       "      <td>New York, NY 10011 (Chelsea area)</td>\n",
       "      <td>3 years of experience working with statistica...</td>\n",
       "      <td>indeed.com/rc/clk?jk=7481daf125074c4a&amp;fccid=a5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst Intern</td>\n",
       "      <td>Chexology</td>\n",
       "      <td>New York, NY 10011 (Chelsea area)</td>\n",
       "      <td>Chexology, creator of ABCs Shark Tank featur...</td>\n",
       "      <td>indeed.com/rc/clk?jk=94f2310380ce332f&amp;fccid=45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Crux Informatics</td>\n",
       "      <td>New York, NY 10261</td>\n",
       "      <td>You are extremely attentive to detail.Data an...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0AbcLYJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Harnham US</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>A Technology startup with a fantastic product...</td>\n",
       "      <td>indeed.com/pagead/clk?mo=r&amp;ad=-6NYlbfkN0ARICpN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0              Alcohol Products Data Mapping Analyst   \n",
       "0          Data Analyst, Special Investigations Unit   \n",
       "0                           BVAL Yield Curve Analyst   \n",
       "0                               Data Analyst, CHOICE   \n",
       "0                               Junior Analyst, Data   \n",
       "0                                Junior Data Analyst   \n",
       "0  Full Time Job Opportunity for Data Analyst wit...   \n",
       "0                                  OIRA Data Analyst   \n",
       "0                            Analyst, Data Analytics   \n",
       "0    Digital Data Analyst - SportsNet New York (SNY)   \n",
       "0                                       Data Analyst   \n",
       "0                              Product Analyst, Data   \n",
       "0                                 Data Analyst (NCU)   \n",
       "0                                Talent Data Analyst   \n",
       "0                                   Sr. Data Analyst   \n",
       "0          Healthcare Quality Assurance Data Analyst   \n",
       "0                   Sales Operations, Senior Analyst   \n",
       "0                           Data Analyst, Paid Media   \n",
       "0               Audience Data & Analytics Specialist   \n",
       "0                           Analyst, Data & Analysis   \n",
       "0                                       Data Analyst   \n",
       "0                                       Data Analyst   \n",
       "0                                  Data Analyst, PNA   \n",
       "0                                People Data Analyst   \n",
       "0                   Quantitative/Data Analyst Intern   \n",
       "0                                       Data Analyst   \n",
       "0                Business Data Analyst (Entry Level)   \n",
       "0                                       Data Analyst   \n",
       "0               Product Data Analyst - Equinox Media   \n",
       "0                                       Data Analyst   \n",
       "0                                       Data Analyst   \n",
       "0                                       Data Analyst   \n",
       "0                                   Epic HIM Analyst   \n",
       "0                           Financial - Data Analyst   \n",
       "0                                IT Business Analyst   \n",
       "0                                   EpicCare Analyst   \n",
       "0                 Mortgage Data & Collateral Analyst   \n",
       "0                         Data & Insight Analyst, DK   \n",
       "0                              Investigative Analyst   \n",
       "0                                       Data Analyst   \n",
       "0                                       Data Analyst   \n",
       "0                                    HR DATA ANALYST   \n",
       "0                     Data Analyst, Customer Success   \n",
       "0                     Analyst, Vantage Data Solution   \n",
       "0                           Data Analyst Entry Level   \n",
       "0                      Product Analyst, Data Science   \n",
       "0                                Data Analyst Intern   \n",
       "0                                       Data Analyst   \n",
       "0                                       Data Analyst   \n",
       "\n",
       "                                             company  \\\n",
       "0                                       3x3 Insights   \n",
       "0              EmblemHealth/AdvantageCare Physicians   \n",
       "0                                          Bloomberg   \n",
       "0                 Visiting Nurse Service of New York   \n",
       "0                      HarperCollins Publishers Inc.   \n",
       "0                                            Peloton   \n",
       "0                          KGS Technology Group, Inc   \n",
       "0  Research Foundation of The City University of ...   \n",
       "0                                         CMI/Compas   \n",
       "0                                       NBCUniversal   \n",
       "0                                             Turner   \n",
       "0                                            Reonomy   \n",
       "0             New York City DEPARTMENT OF CORRECTION   \n",
       "0                                   Uncommon Schools   \n",
       "0                                          Noom Inc.   \n",
       "0                       Bridging Access to Care, Inc   \n",
       "0                                             Indeed   \n",
       "0                          Disney Streaming Services   \n",
       "0                                    Investment News   \n",
       "0                                            Digitas   \n",
       "0                                           RedRoute   \n",
       "0                                      UncommonGoods   \n",
       "0                New York City NYC HOUSING AUTHORITY   \n",
       "0                                            Peloton   \n",
       "0                                        KKI Capital   \n",
       "0                                        BazaarVoice   \n",
       "0                                     Medly Pharmacy   \n",
       "0                                       ThreatMetrix   \n",
       "0                                            EQUINOX   \n",
       "0                                      TeamSoft, Inc   \n",
       "0          Integrated Reporting is Simple (IRIS CRM)   \n",
       "0                                 NYU Langone Health   \n",
       "0                                 NYU Langone Health   \n",
       "0                             Renovation Brands, LLC   \n",
       "0                          Worldwide Flight Services   \n",
       "0                                 NYU Langone Health   \n",
       "0                    Global Atlantic Financial Group   \n",
       "0                           Penguin Random House LLC   \n",
       "0                New York District Attorney's Office   \n",
       "0                                          Cedar Inc   \n",
       "0                                          Gallagher   \n",
       "0                                     ABM Industries   \n",
       "0                                       CipherHealth   \n",
       "0                                             Viacom   \n",
       "0                                              Endai   \n",
       "0                                             Google   \n",
       "0                                          Chexology   \n",
       "0                                   Crux Informatics   \n",
       "0                                         Harnham US   \n",
       "\n",
       "                                           city  \\\n",
       "0                                  New York, NY   \n",
       "0  New York, NY 10004 (Financial District area)   \n",
       "0                                  New York, NY   \n",
       "0                            New York, NY 10261   \n",
       "0             New York, NY 10022 (Midtown area)   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0             New York, NY 10017 (Midtown area)   \n",
       "0                                    Queens, NY   \n",
       "0             New York, NY 10018 (Clinton area)   \n",
       "0                                  New York, NY   \n",
       "0                                  Brooklyn, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0             New York, NY 10017 (Midtown area)   \n",
       "0        New York, NY 10014 (West Village area)   \n",
       "0                                  Brooklyn, NY   \n",
       "0         Brooklyn, NY 11220 (Sunset Park area)   \n",
       "0                                 Manhattan, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0        Brooklyn, NY 11206 (Williamsburg area)   \n",
       "0             New York, NY 10118 (Chelsea area)   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0           Brooklyn, NY 11209 (Bay Ridge area)   \n",
       "0            New York, NY 10016 (Gramercy area)   \n",
       "0            New York, NY 10016 (Gramercy area)   \n",
       "0                                  Brooklyn, NY   \n",
       "0                                   Jamaica, NY   \n",
       "0            New York, NY 10016 (Gramercy area)   \n",
       "0                                  New York, NY   \n",
       "0             New York, NY 10019 (Midtown area)   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0                                  New York, NY   \n",
       "0                                 Manhattan, NY   \n",
       "0             New York, NY 10018 (Clinton area)   \n",
       "0                            New York, NY 10036   \n",
       "0  New York, NY 10038 (Financial District area)   \n",
       "0             New York, NY 10011 (Chelsea area)   \n",
       "0             New York, NY 10011 (Chelsea area)   \n",
       "0                            New York, NY 10261   \n",
       "0                                  New York, NY   \n",
       "\n",
       "                                             summary  \\\n",
       "0   Data Mapping Analyst  Part Time.3x3 Insights...   \n",
       "0   3-5 Years of experience in data analytics, pr...   \n",
       "0   Bloomberg's evaluated pricing service, BVAL, ...   \n",
       "0   Compiles, analyzes, and reports on CHOICE hea...   \n",
       "0   HarperCollins is seeking a Junior Analyst to ...   \n",
       "0   As a Junior Data Analyst, you will join the B...   \n",
       "0   *Immigration/VISA Support (International stud...   \n",
       "0   Candidates should have a great attention to d...   \n",
       "0   Keen attention to detail, organization, and s...   \n",
       "0   SNY is looking for a part-time data analyst t...   \n",
       "0   To be successful in the role, you'll need to ...   \n",
       "0   A detail oriented mind with inclination for i...   \n",
       "0   Strong analytical skills with ability to coll...   \n",
       "0   The Associate Director for Talent Data and St...   \n",
       "0   At Noom, we use scientifically proven methods...   \n",
       "0   Working with information systems: 3 years (Re...   \n",
       "0   You must possess strong Excel skills, a good ...   \n",
       "0   Disney Streaming Services (DSS) is looking fo...   \n",
       "0   You are detail oriented and take pride in bei...   \n",
       "0   1-3 years of work experience in a relevant in...   \n",
       "0   RedRoute is paving the path to amazing voice-...   \n",
       "0   Analytics works with numerous departments acr...   \n",
       "0   Analyze financial and investment opportunitie...   \n",
       "0   Excellent attention to detail, process orient...   \n",
       "0   Collect, validate and organize data per the t...   \n",
       "0   Superior organizational skills and attention ...   \n",
       "0   Were looking for an entry-level data analyst...   \n",
       "0   The fraud data analyst works as part of a cus...   \n",
       "0   Attention to detail and accuracy.Product Data...   \n",
       "0   Policies and claims: 5 years (Preferred).Data...   \n",
       "0   On a scale of 1 to 5, with 5 being expert, ho...   \n",
       "0   Excellent written and oral communication skil...   \n",
       "0   Requires 1-3 years of relevant experience and...   \n",
       "0   3 years experience in within a Finance or Ac...   \n",
       "0   MS Power BI: 5 years (Required).Microsoft Acc...   \n",
       "0   Requires 1-3 years of relevant experience and...   \n",
       "0   Ability to learn quickly with a strong attent...   \n",
       "0   Excellent attention to detail and time manage...   \n",
       "0   Analyze documentary evidence, including cell-...   \n",
       "0   Cedar is a patient payment and engagement pla...   \n",
       "0   Gallagher is a global leader in insurance, ri...   \n",
       "0   Outstanding analytical and organizational ski...   \n",
       "0   We are a fast-growing company in the healthca...   \n",
       "0   2-5 years working experience in data analytic...   \n",
       "0   Type: Paid Intern (in a farm team).Education:...   \n",
       "0   3 years of experience working with statistica...   \n",
       "0   Chexology, creator of ABCs Shark Tank featur...   \n",
       "0   You are extremely attentive to detail.Data an...   \n",
       "0   A Technology startup with a fantastic product...   \n",
       "\n",
       "                                                href  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AOP7UQ...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CkHVnD...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ab67y_...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0B2LP6K...  \n",
       "0  indeed.com/rc/clk?jk=977d1aa7722eafeb&fccid=6b...  \n",
       "0  indeed.com/rc/clk?jk=737c4d597c481e55&fccid=88...  \n",
       "0  indeed.com/company/KGS-Technology-Group-Inc/jo...  \n",
       "0  indeed.com/rc/clk?jk=5bb807b6ecf2ec25&fccid=66...  \n",
       "0  indeed.com/rc/clk?jk=d6253d34d6fa1a48&fccid=7d...  \n",
       "0  indeed.com/rc/clk?jk=2c7f0d512362ff79&fccid=35...  \n",
       "0  indeed.com/rc/clk?jk=dbb0ebab7b2dab52&fccid=92...  \n",
       "0  indeed.com/rc/clk?jk=852c306a7c769890&fccid=ca...  \n",
       "0  indeed.com/rc/clk?jk=e1920b5544f31cee&fccid=68...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CApKCi...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CTxeck...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0C7-oBx...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CiRNM7...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0C-wIR9...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DoqYCt...  \n",
       "0  indeed.com/rc/clk?jk=c0708d39bef9d2eb&fccid=27...  \n",
       "0  indeed.com/company/RedRoute/jobs/Data-Analyst-...  \n",
       "0  indeed.com/rc/clk?jk=28d611e84addb020&fccid=5b...  \n",
       "0  indeed.com/rc/clk?jk=fa7fdea1e7ea4c08&fccid=96...  \n",
       "0  indeed.com/rc/clk?jk=39737f1c09659831&fccid=88...  \n",
       "0  indeed.com/rc/clk?jk=018ed0ca7a7ce1a2&fccid=2b...  \n",
       "0  indeed.com/rc/clk?jk=5d77612f8cc80067&fccid=e2...  \n",
       "0  indeed.com/company/Medly-Pharmacy/jobs/Busines...  \n",
       "0  indeed.com/company/ThreatMETRIX/jobs/Data-Anal...  \n",
       "0  indeed.com/rc/clk?jk=538ef7b5ac072ac9&fccid=8e...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CDnM5s...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0C7ZzwE...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BgHAp2...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BgHAp2...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AspZ6W...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CFS0Jg...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BgHAp2...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CttO8m...  \n",
       "0  indeed.com/rc/clk?jk=143462ad92078a27&fccid=2e...  \n",
       "0  indeed.com/rc/clk?jk=e45248ae06a1eb53&fccid=db...  \n",
       "0  indeed.com/rc/clk?jk=58a71fc749a37a28&fccid=67...  \n",
       "0  indeed.com/rc/clk?jk=91abbd950fc6d0e8&fccid=34...  \n",
       "0  indeed.com/rc/clk?jk=5a5e37b51590202f&fccid=dc...  \n",
       "0  indeed.com/rc/clk?jk=87e0676abf63f477&fccid=11...  \n",
       "0  indeed.com/rc/clk?jk=fb90d6ff1a9b2346&fccid=ae...  \n",
       "0  indeed.com/rc/clk?jk=a630d8604e79813a&fccid=f7...  \n",
       "0  indeed.com/rc/clk?jk=7481daf125074c4a&fccid=a5...  \n",
       "0  indeed.com/rc/clk?jk=94f2310380ce332f&fccid=45...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AbcLYJ...  \n",
       "0  indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0ARICpN...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
